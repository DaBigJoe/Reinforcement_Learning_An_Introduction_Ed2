\section{Finite Markov Decision Processes}

\subsection*{Exercise 3.1: Three MDP Examples}

\begin{enumerate}
	\item \textbf{Stock Market Agent} Buying and selling shares on the stock market or cryptocurrency exchanges could be presented as a finite MDP. The states would be the various stock/currency information such as opening price, closing price, high prices and low prices for some period of time. The possible actions would be to buy and sell various assets, with the rewards being an increase or decrease in overall value of the currently held assets.
	\item \textbf{Football Player} This could be presented as an individual player (e.g. a striker) or a `team controller' as seen in video games such as Fifa. This example focuses on the latter case, as the former could be considered a partially observable MDP. The state would be the current player positions, the ball position, and statistics such as the score and previous opposition tactics (to capture everything previously seen about the game and thus satisfy the Markov property). The actions would be a vector of commands for each player such as pass here, run here, shoot etc. The obvious reward is the overall score, however this is too sparse a reward to be of any use. More fine-grained rewards such as completing a successful pass and getting a shot on target would make it easier to learn how to play, however the reward function would need to be carefully constructed such that the agent doesn't just pass the ball between defenders as this gives reward without much risk of losing possession.
	\item \textbf{Spam Detection} The problem of spam detection, although much less of an issue in recent times, could be presented as a MDP. The states would be the incoming messages, along with a history of which messages this particular user had considered spam and which they hadn't. The agent only has two actions - mark a message as spam or allow it to enter the user's inbox. One way to formulate the reward function is to give a negative reward for every email that enters the user's inbox and is subsequently marked as spam - this represents a message `slipping through'. Only the other hand, the agent could incorrectly identify messages as spam that are actually important to the user, this would result in a negative reward if the user marks a message as not spam.   
\end{enumerate}

\subsection*{Exercise 3.2: MDP Limitations}

Beginning with states, not all possible states will satisfy the Markov property - there might be information about previous states that aren't represented in the current state, for example an important object passing out of site of an agent's sensors will appear to no longer to exist unless the agent stores information regarding the previous states in some form of memory. This assumes the agent's memory is under complete control of the agent, viz. it is not part of the environment. This leads to an extension of the MDP framework - the partially observable MDP - in which the state the agent receives from the environment is not the complete state of the environment. \\

With actions, the agent is limited based on the rules of the environment. This is fine in most cases if the action set is well designed, however more optimal strategies could be missed if an agent is given an incomplete action set. For example, if an agent only has the actions \textit{move left} and \textit{move right}, the agent is forced to move on every step therefore it might not find the best strategy if it could benefit from not moving in certain states. Although this is a trivial case, when applying RL to more complex problems it may be beneficial to make the agent able to take actions that weren't originally considered in the problem specification, as this will allow the agent to find more optimal strategies. \\

Finally, a limitation of the rewards used in MDPs is that they are restricted to a single number. This is fine when optimising for a single objective, but often it is useful to optimise for two or more objectives, usually when looking for a trade-off between different criteria. While it is possible to condense any number of objectives into a single number, this brings additional parameters that need tuning to specify the importance of each different reward function. Although not considered `true' RL methods by the text, genetic algorithms can be used for multi-objective optimisation to produce a range of Parteo-optimal solutions to a problem, thus highlighting a potential advantage they have over methods that just solve MDPs as outlined in the text.